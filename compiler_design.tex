\documentclass[12pt, a4paper]{article}

% Preamble

\usepackage[margin=1in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{cprotect}
\usepackage{enumerate}
\usepackage{pdfpages}

\newcommand{\para}[1]{\paragraph{#1}\mbox{}\\}
\newcommand{\icode}{\texttt}

\title{Compiler Design Document}
\date{\today}
\author{Richard Peng, Jane Wang}
% Document

\begin{document}

\maketitle

\section{Introduction}

For the first stage of our compiler, we wanted to create a general purpose scanner and parser, that's not specifically tied to Joos 1W rules. We've built these into Python modules that contain core data structures, as well as generic scanning and parsing algorithms, such as Maximal Munch and Shift-Reduce. A separate module contains all of the Joos specific constructs. \\

We wrote our compiler in Python 2.7, and used GitHub to host the project repository. The report will first give an overview of the directory layout of the repository, the overall architecture of our compiler, and then a description of each component. Finally, it will discuss some of the challenges that we faced during planning and implementations, and our solutions to these challenges.

\section{Directory Structure}

\subsection{\texttt{compiler/}}

This module contains a Maximum Munch scanner, a Shift-Reduce parser, as well as defines a few errors that may arise - for example when the scanner encounters an invalid token, or the parser is expecting a different one.

\subsection{\texttt{extern/} (Omitted from submissions)}

Contains external helpers that were used, such as the Jlalr parse table generator, and the Marmoset test files.
 
\subsection{\texttt{generated/}}

Contains generated files that are not placed under version control. This includes a \verb|joos.cfg| (more about how this was generated later), as well as a \verb|joos.lr1| (the output from the Jlalr generator).

\subsection{\texttt{joos/}}

All of the Joos 1W specific modules are put here. Contains Joos grammar files, the beginnings of an Abstract Syntax, regular expressions for Joos 1W tokens, as well as some Joos specific visitors that will aid in building the AST and weeding.

\subsection{\texttt{structs/}}

Core data structures that are required for many functions. This includes Finite State Machines, Regular Expressions, and Context-Free Grammars.

\subsection{\texttt{scripts/}}

Contains a helper script that can convert a custom CFG specification format to one that is accepted by the Jlalr token generator.


\subsection{\texttt{./joosc}}

Executable program entry point. Takes in a path to a Joos file as an argument, and returns 0 if it is valid Joos, 42 if it is not.

\section{Compiler Architecture}

As advised, we divided our compiler into three stages: Scanning, Parsing and Weeding. The scanner takes in a source file and outputs tokens. The parser then takes the tokens and builds a derivation in the form of a parse tree. Finally, the weeder traverses the parse tree and validates each node. We allowed the grammar to be  less strict (for example, when evaluating modifiers), as to prevent conflicts in the LALR(1) machine. The weeding step is necessary to validate constraints that were omitted from the grammar.

\subsection{Scanning}

Before the scanner is invoked, all the state machines required for parsing Joos tokens are created. These state machines are generated by composition of regular expressions. Our regular expressions are simply NFAs, as well as some operators that can act on regular expressions - such as concatenation, union, optional, one of, zero-or-more. A regular expression is created for each keyword. Literals and identifiers have special regular expressions that matches Java literals (integers, string, etc...) and identifiers (name, etc). There is a list of all the exported tokens (in \verb|exports.py|), listed in the order of priority, which is used by the scanner. The priority is important as it will allow us to distinguish between multiple accepting expressions, for example keywords and identifiers. \\

The scanner takes a map (from token types to a regular expression corresponding to that type), as well as an input file as arguments, and generates token types using the Maximal Munch algorithm. It accepts input, and runs all of the state machines until they've reached an error state. It will then rewind to the last accepting state and emit the associated token type from the map. Tokens are annotated with the lexeme, token type, and line and row columns that they appeared in the source file. \\

An error occurs whenever the scanner is in an error state, and there is no last accepting state. In this case, we output an error, the lexeme that is currently being built, as well as row and column numbers of the error.

\subsection{Parsing}

The parser takes in the tokens produced by the scanner, a parse table, and builds a parse tree using Shift-Reduce.

\subsubsection{Grammar (jcfg)}
To generate the parse table, we first described the Joos 1W grammar(see \verb|joos/grammar/|). We used a custom specification format (dubbed jcfg) that allowed us to write comments, split up the grammar into separate files, and use cleaner syntax to describe rules $A \rightarrow E + S$. \\

We also implemented support to expand optionals, so a rule such as $A \rightarrow E? + S + B?$ would expand to:

\begin{align*}
A & \rightarrow E + S + B  \\
A & \rightarrow + S + B \\
A & \rightarrow E + S + \\
A & \rightarrow + S + \\
\end{align*}

This format was deemed more suitable than the raw \verb|cfg| format, as made it much easier to write and maintain our grammar. \\

Finally, after creating our rules, we have a tool that takes them, expands the optionals, and outputs a \verb|cfg| file. This file is sent to the \verb|Jlalr| tool for processing, and the \verb|lr1| output is saved. \\


\subsubsection{Preparation}
Before we use the parser, we perform a few steps to prepare the tokens for parsing. We remove whitespace, comments, and newline tokens, and inject a Beginning-of-File (BOF) token at the front, as well as an End-of-File (EOF) at the end. This is required for the rules to perform correctly. \\

At this step, we also screen the tokens for unsupported keywords and operators (defined in Java 1.3 but not supported in Joos 1W). Whenever we encounter such a token, we emit an error, with the token's lexeme, and row and column numbers.

\subsubsection{Shift Reduce}

We first generate the parse table from \verb|lr1| file that is produced by \verb|Jlalr|, and use that as the basis for our Shift-Reduce algorithm. As each input token is shifted, we create a terminal parse tree node for that token. Whenever a reduce action occurs, we pop n elements corresponding to the right-hand-side of the production rule, and create a new parse tree node with the rule's left hand side as the parent, and the popped elements as the children. The rule is also stored in the tree node for context. \\

An error occurs whenever a token does not correspond to a shift rule or a reduce rule. The parser would output the error, token and lexeme, row and column numbers, as well as the expected values for the next token. \\

Finally, when the parser shifts the EOF token, the parse tree has been fully created and is returned.

\subsection{Weeding}

The grammar rules that were provided to the parser were a relaxed version of the Joos 1W specification. The weeder checks that source files that were accepted by the language actually conforms to the specifications.

\subsubsection{Abstract Syntax}
Before we start weeding, we wanted to build an abstract syntax tree for the parse tree nodes that need to be checked (for example, class declarations, and method declarations). To do this, we used the Visitor pattern to traverse the concrete tree provided by the parser. \\

We created a BuilderVisitor that traverses the concrete tree. Whenever this visitor encounters a node whose rule's LHS matches a structure that needs to be checked, we create an abstract syntax object for that node. These objects contain only the essentials for that construct - no semicolons, brackets, or anything that is only there for parsing. If the node does not match one of our structures, the visitor leaves it alone and keeps the concrete tree. \\

At the end of this step, a partial abstract syntax tree is generated. This tree will be used as the basis for the weeding step.

\subsubsection{Weeding}

Our final step is the Weeding step. The weeder is another visitor that traverses the abstract syntax tree. Whenever it encounters an abstract syntax node, the corresponding weed action is called. \\

We check the rules that were not enforced by the grammar, and the ones that are explicitly listed on the Assignment 1 webpage. We also run the compiler against an integration test suite that's provided by the course.


\section{Challenges}

We faced several challenges when designing and implementing our compiler. The following is a summary of the challenges, as well as the steps we took in addressing them.

\subsection{Ensuring Correctness}

As the compiler is a fairly large project, we needed to make sure that it performed correctly at each step of the way. We tackled this early on by writing unit tests for each small component that we build. Each time we add a new feature, we also add corresponding tests and re-run the test suite. As these tests run quickly, we can easily check if we introduced any regressions every step of the way. \\

We also downloaded the Assignment 1 Marmoset tests, and run our compiler against these once we have completed the parser. This helped us decide which parts to structures we want to convert to abstract syntax for weeding. \\

Finally, we use source control (Git) to help us track our progress, and revert in case anything goes wrong. We push our code to a private GitHub repository in case any catastrophic accidents cause us to lose our data locally.

\subsection{Maintainability}

It becomes hard to maintain a large project if it is not organized. We took steps from the beginning to structure our compiler so that it is very clear what each component does, and what the dependencies are. This also made it easy to write tests. \\

Finally, we also determined that writing a single large \verb|cfg| file to feed to the \verb|Jlalr| parse table generator was not the way to go. That prompted us to develop our own CFG specification format and tool, which really helped accelerate the development of the Joos 1W grammar.

\end{document}
